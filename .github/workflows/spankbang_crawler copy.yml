name: SpankBang Crawler

on:
  schedule:
    - cron: '0 2/4 * * *'  # 从凌晨2点开始，每4小时运行一次
  workflow_dispatch:
    inputs:
      command:
        description: '命令类型'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - video
          - search
          - batch
      start_page:
        description: '起始页码'
        required: false
        default: '1'
        type: string
      end_page:
        description: '结束页码'
        required: false
        default: '20'
        type: string
      url:
        description: '视频URL'
        required: false
        type: string
      query:
        description: '搜索关键词'
        required: false
        type: string
      pages:
        description: '搜索页数'
        required: false
        default: '3'
        type: string
      file:
        description: 'URL文件'
        required: false
        default: 'urls.txt'
        type: string
      use_proxy:
        description: '是否使用代理'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'
      retry_count:
        description: '最大重试次数'
        required: false
        default: '3'
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 设置6小时超时
    
    steps:
    - name: 检出代码
      uses: actions/checkout@v4

    - name: Checkout private repository
      uses: actions/checkout@v4
      with:
        repository: yjq001/sex-robot
        path: ./sex-robot
        token: ${{ secrets.PAT_TOKEN }}
        ref: master
        fetch-depth: 1
        persist-credentials: false

    - name: Copy files from private repository
      run: |
        cp -r ./sex-robot/* ./
      
    - name: 设置Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: 安装依赖
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # 安装绕过Cloudflare的库
        pip install cloudscraper undetected-chromedriver selenium-stealth
        
    - name: 设置和启动Selenium容器
      run: |
        # 增加容器共享内存和配置反检测参数
        docker run -d --name selenium -p 4444:4444 --shm-size=4g \
          -e SE_NODE_MAX_SESSIONS=2 \
          -e SE_NODE_OVERRIDE_MAX_SESSIONS=true \
          -e SE_OPTS="--disable-dev-shm-usage --no-sandbox --disable-gpu" \
          selenium/standalone-chrome:latest
          
        # 等待Selenium容器启动
        echo "等待Selenium容器启动..."
        sleep 15
        
        # 检查容器是否正常运行
        docker ps | grep selenium
        docker logs selenium
        
    - name: 设置随机User Agent
      id: useragent
      run: |
        USER_AGENTS=(
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36"
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36"
          "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15"
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0"
        )
        RANDOM_INDEX=$((RANDOM % ${#USER_AGENTS[@]}))
        RANDOM_UA="${USER_AGENTS[$RANDOM_INDEX]}"
        echo "RANDOM_UA=$RANDOM_UA" >> $GITHUB_ENV
        echo "选择的随机UA: $RANDOM_UA"
        
    - name: 运行爬虫
      env:
        PYTHONUNBUFFERED: 1
        SELENIUM_REMOTE_URL: http://localhost:4444/wd/hub
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        HTTP_PROXY: ${{ github.event.inputs.use_proxy == 'true' || github.event_name != 'workflow_dispatch' ? secrets.HTTP_PROXY : '' }}
        HTTPS_PROXY: ${{ github.event.inputs.use_proxy == 'true' || github.event_name != 'workflow_dispatch' ? secrets.HTTPS_PROXY : '' }}
        USER_AGENT: ${{ env.RANDOM_UA }}
        CLOUDSCRAPER_ENABLED: "1"
      run: |
        # 确保脚本可执行
        chmod +x *.py
        
        # 等待一段随机时间，避免被检测
        RANDOM_WAIT=$((RANDOM % 45 + 15))
        echo "随机等待 $RANDOM_WAIT 秒..."
        sleep $RANDOM_WAIT
        
        # 查看脚本帮助信息
        python ./spankbang_crawler.py --help
        
        # 准备代理参数
        PROXY_OPTION=""
        if [ -n "$HTTP_PROXY" ]; then
          PROXY_OPTION="--proxy $HTTP_PROXY"
          echo "使用代理: $HTTP_PROXY"
        else
          echo "不使用代理"
        fi
        
        # 添加IP旋转和UA轮换
        echo "当前外部IP信息："
        curl -s https://ipinfo.io || echo "无法获取IP信息"
        
        # 创建临时脚本修改WebDriver
        cat > patch_cloudflare.py << 'EOF'
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def patch_selenium_options():
    """向selenium_script.py添加额外的选项绕过Cloudflare"""
    with open('spankbang_scraper.py', 'r') as f:
        content = f.read()
    
    if 'chrome.options.add_experimental_option("excludeSwitches", ["enable-automation"])' not in content:
        patch = '''
    # 添加绕过Cloudflare检测的选项
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument("--disable-blink-features=AutomationControlled")
    
    # 增加等待时间，处理Cloudflare验证
    if "SELENIUM_REMOTE_URL" in os.environ:
        from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
        caps = DesiredCapabilities.CHROME.copy()
        caps['pageLoadStrategy'] = 'normal'  # 等待页面完全加载
        driver = webdriver.Remote(
            command_executor=os.environ["SELENIUM_REMOTE_URL"],
            desired_capabilities=caps,
            options=options
        )
        driver.set_page_load_timeout(60)  # 设置页面加载超时为60秒
        return driver
        '''
        
        # 插入补丁
        import re
        patched_content = re.sub(r'(def _init_driver.*?\n.*?return driver\n)', r'\1' + patch, content, flags=re.DOTALL)
        
        with open('spankbang_scraper.py', 'w') as f:
            f.write(patched_content)
        
        print("成功添加绕过Cloudflare检测的选项")
    else:
        print("已包含Cloudflare绕过选项")

if __name__ == "__main__":
    patch_selenium_options()
EOF

        # 应用临时补丁
        python patch_cloudflare.py
        
        # 重试机制
        MAX_RETRIES=${{ github.event.inputs.retry_count || '3' }}
        RETRY_COUNT=0
        SUCCESS=false
        
        while [ $RETRY_COUNT -lt $MAX_RETRIES ] && [ "$SUCCESS" = "false" ]; do
          RETRY_COUNT=$((RETRY_COUNT + 1))
          echo "尝试运行爬虫 (尝试 $RETRY_COUNT/$MAX_RETRIES)..."
          
          # 根据输入参数运行爬虫
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            # 手动触发时使用输入参数
            if [ "${{ github.event.inputs.command }}" = "full" ]; then
              python ./spankbang_crawler.py --headless $PROXY_OPTION full --start-page ${{ github.event.inputs.start_page }} --end-page ${{ github.event.inputs.end_page }}
              EXIT_CODE=$?
            elif [ "${{ github.event.inputs.command }}" = "video" ]; then
              python ./spankbang_crawler.py --headless $PROXY_OPTION video "${{ github.event.inputs.url }}"
              EXIT_CODE=$?
            elif [ "${{ github.event.inputs.command }}" = "search" ]; then
              python ./spankbang_crawler.py --headless $PROXY_OPTION search "${{ github.event.inputs.query }}" --pages ${{ github.event.inputs.pages }}
              EXIT_CODE=$?
            elif [ "${{ github.event.inputs.command }}" = "batch" ]; then
              python ./spankbang_crawler.py --headless $PROXY_OPTION batch --file ${{ github.event.inputs.file }}
              EXIT_CODE=$?
            fi
          else
            # 自动触发时使用默认参数
            python ./spankbang_crawler.py --headless $PROXY_OPTION full --start-page 1 --end-page 20
            EXIT_CODE=$?
          fi
          
          if [ $EXIT_CODE -eq 0 ]; then
            SUCCESS=true
            echo "爬虫成功完成!"
          else
            echo "爬虫运行失败，退出代码: $EXIT_CODE"
            if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
              # 指数退避策略
              SLEEP_TIME=$((30 * 2**$RETRY_COUNT))
              echo "等待 $SLEEP_TIME 秒后重试..."
              sleep $SLEEP_TIME
              
              # 随机切换 User Agent
              RANDOM_INDEX=$((RANDOM % ${#USER_AGENTS[@]}))
              RANDOM_UA="${USER_AGENTS[$RANDOM_INDEX]}"
              echo "RANDOM_UA=$RANDOM_UA" >> $GITHUB_ENV
              echo "切换到新的 User Agent: $RANDOM_UA"
            fi
          fi
        done
        
        if [ "$SUCCESS" = "false" ]; then
          echo "所有重试失败，爬虫未成功完成"
          exit 1
        fi
      
    - name: 上传错误日志和调试信息
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: error-logs-spankbang
        path: |
          error_urls_*.txt
          crawler.log
          scraper.log
          page_content_*.html
          
    - name: 清理Docker容器
      if: always()
      run: |
        docker stop selenium || true
        docker rm selenium || true 
