name: SpankBang Crawler

on:
  schedule:
    - cron: '0 2/4 * * *'  # 从凌晨2点开始，每4小时运行一次
  workflow_dispatch:
    inputs:
      command:
        description: '命令类型'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - video
          - search
          - batch
      start_page:
        description: '起始页码'
        required: false
        default: '1'
        type: string
      end_page:
        description: '结束页码'
        required: false
        default: '20'
        type: string
      url:
        description: '视频URL'
        required: false
        type: string
      query:
        description: '搜索关键词'
        required: false
        type: string
      pages:
        description: '搜索页数'
        required: false
        default: '3'
        type: string
      file:
        description: 'URL文件'
        required: false
        default: 'urls.txt'
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: 检出代码
      uses: actions/checkout@v4

    - name: Checkout private repository
      uses: actions/checkout@v4
      with:
        repository: yjq001/sex-robot
        path: ./sex-robot
        token: ${{ secrets.PAT_TOKEN }}
        ref: master
        fetch-depth: 1
        persist-credentials: false

    - name: Copy files from private repository
      run: |
        cp -r ./sex-robot/* ./
      
    - name: 设置Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: 安装依赖
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: 设置和启动Selenium容器
      run: |
        docker run -d --name selenium -p 4444:4444 --shm-size=2g selenium/standalone-chrome:latest
        # 等待Selenium容器启动
        sleep 5
        
    - name: 运行爬虫
      env:
        PYTHONUNBUFFERED: 1
        SELENIUM_REMOTE_URL: http://localhost:4444/wd/hub
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        HTTP_PROXY: ${{ secrets.HTTP_PROXY }}
        HTTPS_PROXY: ${{ secrets.HTTPS_PROXY }}
      run: |
        # 确保脚本可执行
        chmod +x *.py
        
        # 查看脚本帮助信息
        python ./spankbang_crawler.py --help
        
        # 根据输入参数运行爬虫
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          # 手动触发时使用输入参数
          if [ "${{ github.event.inputs.command }}" = "full" ]; then
            python ./spankbang_crawler.py --headless full --start-page ${{ github.event.inputs.start_page }} --end-page ${{ github.event.inputs.end_page }}
          elif [ "${{ github.event.inputs.command }}" = "video" ]; then
            python ./spankbang_crawler.py --headless video "${{ github.event.inputs.url }}"
          elif [ "${{ github.event.inputs.command }}" = "search" ]; then
            python ./spankbang_crawler.py --headless search "${{ github.event.inputs.query }}" --pages ${{ github.event.inputs.pages }}
          elif [ "${{ github.event.inputs.command }}" = "batch" ]; then
            python ./spankbang_crawler.py --headless batch --file ${{ github.event.inputs.file }}
          fi
        else
          # 自动触发时使用默认参数
          python ./spankbang_crawler.py --headless full --start-page 1 --end-page 20
        fi
      
    - name: 上传错误日志
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: error-logs
        path: |
          error_urls_*.txt
          crawler.log
          scraper.log 
